# Conceptual

## 1
**A**. 
Given the sample size n is extremely large and the number of predictors p is small, I would expect a flexible learning method to outperform an inflexible method. Since n is large, there wouldn't be a need to make any assumptions about f(x), which is the first step in an inflexible (parametric) learning method; the very size of n would give structure to f(x) and allowing the flexible method (non-parametric) to learn from the data without restrictions would produce a better model of the data, with little risk of overfitting due to large n.

**B**.
Given the numbers of predictors p is extremely high and the number of observations n is small, I would expect an inflexible method to outperform a flexible one. A flexible method runs the risk of modeling the noise in the data, leading to overfitting and a high test error rate. If some assumptions are made about the structure of f(x) (i.e. using a parametric/inflexible method), the risk of overfitting is lowered.

**C**.
Given the relationship between the predictors and the response is highly non-linear, I would expect a flexible method to outperform an inflexible method since the true f(x) is non-linear, and flexible learning methods don't make any assumptions about the f(x).

**D**.
Given that the variance of the error terms is extremely high, I would expect an inflexible method to outperform a flexible one. Without making any assumptions about f(x) (inflexible/parametric), a flexible method has a relatively high probability of modeling the variance of the error terms as a part of f(x), whereas an inflexible method would reduce that probability, at least somewhat.


## 2
**A**.
Since the question is interested in understanding which factors affect CEO salary, which is a quantitative metric, this would be a regression scenario and the word "understanding" lends towards an interest in inference, as opposed to prediction. n = 500 (firms) and p = 3 (profit, number of employees and industry).

**B**.
Determining whether a new product will be a success or failure is a classification problem, and since the response is binary, logistic regression would be a good starting (Although logistic regression has the word regression in it, due to the output of f(x) being quantitative, the output is a probability of an observation being classified as a success, therefore the method is ultimately used for classification problems). n = 20 (similar products) and p = 13 (price charged for product, marketing budget, competition price and 10 other variables).  

**C**.
Interest in prediction, and since metric of interest is quantitative, this would be a regression problem. n = 52 (weeks in 2012) and p = 3 (% change in the US market, % change in the British market and the % change in the German market.)
  

## 3
**A**.
(see graphic)

**B**.

* *The Bayes error curve*
(dashed black line) will always be a flat line because it represents the irreducible error; even the best model cannot account for the natural variance of the error terms, hence the horizontal line.

* *The training error curve*
(solid red line) has downward sloping curve because, as more flexible learning methods are used, the method can always "explain away" all of the variance in the training data.

* *The test error curve* (solid blue line)
has a U-shape to it because, as more flexible methods are used, the test error is reduced *up until the point (red circle)* where the method begins to model the noise in the training data, which will lead to higher variance and therefore a higher test error rate. Note that the perfect method and model **cannot** go below the Bayes error curve.

* *The (squared) bias curve*
(dotted black line) has a downward sloping curve. Bias is defined as the error that occurs by attempting to model a real life relationship mathematically. So, the simpler (less flexible) the method, the more bias will be introduced (e.g. the probability that any real life relationship is *truly* linear is low). The more flexible the method, the closer the estimate of f(x) will be to the true f(x). (given a large enough sample size)

* *The variance curve*
(solid black line) has is upward sloping because as the flexiblility of the learning method increases, the more the data affect the estimate of f(x), therefore changing the data would change the estimate of f(x) drastically. On the other hand, using the least flexible method possible (horizontal line) as your way of estimating f(x), would have zero variance.
      
      
## 4
**A**.
Classification would be useful when ...

* *Situation 1* ... you would want to determine which treatment method a new cancer patient would respond the best too. The response would be whether the patient went into remission and the predictors would include variables such as age, prevalence of cancer in family history, diet, genetic predispositions, etc (certainly not an easy task). The goal would be prediction, since you theoretically already know the patient has cancer and you would just like to predict the treatment for which they would respond the best.

* *Situation 2* ... you would want to determine whether a potential investment property would be a success or failure (Defining what a "successful" investment property is would be the important part). Given that we define a "successful" investment property as one that makes $10,000/year, the response would be whether other investment properties met that criteria, coded as 1 for success or 0 for failure, and the predictors could be location, rent per month, HOA fees, proximity to parks/malls, etc. Since the ultimate goal would be to decide whether you wanted to invest in the property or not, this would be a prediction problem.

* *Situation 3* ... you want to see how different demographic variables affect someone's voting tendencies. Variables could include age, state of current residence, birth state, voting tendencies of parents, income, married, etc. The responce would be Democrate or Republican. Since the end goal is to see *how* each of the variables affect someone's voting tendencies, this would be an inference problem.

**B**.
Regression would be useful when ...

* *Situation 1* ... you would like to understand how variables affect a customers ecommerce order amount. The response would be the total amount their "cart" was worth at the time of checkout and predictors could be advertising budget, date, the items in their cart and time spent at each webpage. The goal would be inference so as to *understand* how each variable affects the response.

* *Situation 2* ... you would like to predict the next price of a certain stock. Predictors would include the last price, current assets, current liabilities, NCAVPS, prevalence of company in news (to name a few) and the response would be the stocks price. Since the goal is to determine the next price, this would be a prediction problem.

* *Situation 3* ... you want to see how various health habits affect a person resting heart rate. Variables could include diet, time spent exercising per week, type of exercise, age, activity level at job and the reponse would be their resting heart rate. Sinc predicting someone's resting heart rate isn't of that much importantance, this would be an inference problem; you are looking to understand *how* each variable affects the response.

**C**.

Cluster analysis would be useful when ...

* *Situation 1* ... a company would like to see what similarities their customers have with one another. Clustering could be performed on location, income level, education, etc.

* *Situation 2* ... clinical researchers would like to see what similarities patients with Alzheimer's disease have.

* *Situation 3* ... the authors of "An Introduction to Statistical Learning" want to see the various education levels of the people that bought their book. 


## 5

The advantages of a flexible learning method over a less flexible learning method are that flexible learning methods are typically going to be less biased than inflexible method (holding all else equal). However, with this comes the disadvantage that more flexible method are also prone to more variance. In addition, flexible methods are usually much less interpretable than inflexible methods, so if inference is the goal, an inflexible method might be a better option. If prediction is the goal and it doesn't particularly matter if you understand how each variable affects the outcome, a more flexible method will most likely be more accurate.


## 6

The main difference between parametric and non-parametric methods is that parametric methods make some assumption about the general shape of f(x). The advantages of this are that, give that the structure is defined, the challenge of estimating f(x) is reduced to one of estimating the parameters/coefficients of each regressor. On the other hand, non-parametric models make no assumption about the structure of f(x). For both regression and classification implementations of non-parametric methods, a large sample size is needed to reduce the chance of overfitting, so when n is small, a parametric approach is probably the better option.


## 7
**A**.
```{r}
# create data frame
obs <- c(1,2,3,4,5,6)
x1 <- c(0,2,0,0,-1,1)
x2 <- c(3,0,1,1,0,1)
x3 <- c(0,0,3,2,1,1)
y <- c('red', 'red', 'red', 'green' ,'green', 'red')
df <- data.frame(Obs = obs, v1 = x1, v2 = x2, v3 = x3, response = y)

suppressPackageStartupMessages(library(dplyr))

x <- (df$v1 - 0)^2
y1 <- (df$v2 - 0)^2
z <- (df$v3 - 0)^2
df1 <- as.data.frame(cbind(x,y1,z))
df2 <- mutate(df1, total = rowSums(df1))
df3 <- mutate(df2, Distance = round(sqrt(total), digits = 2))
df3 <- cbind(obs, df3, y)
arrange(df3, Distance)
```

**B**.
When K = 1, the prediction for Y would be green because the "nearest neighbor" to the test point (at a distance of 1.41) is observation 5, which is green.

**C**.
When K = 3, the 3 "nearest neighbors" to the test point are green, red and red. Therefore, P("Green" | X = 0) = 0.33 and P("Red" | X = 0) = 0.66 and the test point is assigned the classification with the highest probability, red.

**D**.
If the Bayes Decision Boundary is highly non-linear, then you could expect the best (most accurate) value for K to be small. The reason for this is as K increases, the Bayes Decision Boundary becomes less and less flexible (i.e. more and more linear) because, as the classifier takes more data points into consideration, each data point has less leverage on the decision boundary.




# Applied
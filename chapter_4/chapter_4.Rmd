# Conceptual

## 1

$$
P(X)~=~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}~~~and~~~\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{P(X)} {1~-~P(X)}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}} {1~-~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1 + \epsilon^{\beta_0~+~\beta_1X_1}}}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}} {\frac {1~+~\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}~-~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1 + \epsilon^{\beta_0~+~\beta_1X_1}}}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}} {\frac {1} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}}
$$


$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}~\dot~\frac {1~+~\epsilon^{\beta_0~+~\beta_1X_1}} {1}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\epsilon^{\beta_0~+~\beta_1X_1}
$$

## 2

Prove that maximizing the equation

$$
p_k(x)~=~\frac{\pi_k\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}} {\sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}}
$$
for $k$, is equivalent to maximizing the equation

$$
\delta_k(x)~=~\frac{x\mu_k}{\sigma^2}~-~\frac{\mu_k^3}{2\sigma^2}~+~log(\pi_k)
$$
for $k$.

As the text says, taking the $log$ of the first equation will get us started.

$$
log\left( p_k(x) \right)~=~log\left( \frac{\pi_k\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}} {\sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}}\right)
$$

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{1}{2\sigma^2}\left( x~-~\mu_k^2 \right) ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
Expanding the third term in the equation we get

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{1}{2\sigma^2}\left( x^2~-~2x\mu_k~+~\mu_k^2 \right) ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
Once again expanding the third term, this time multiplying the $~-~\frac{1}{\sqrt{2\sigma62}}$ with all terms within the parenthesis, we get

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{x^2} {2\sigma^2} ~+~
\frac{2x\mu_k} {2\sigma^2} ~-~
\frac{\mu_k^2}{2\sigma^2} ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
simplifying what is now the fourth term in equation, we get

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{x^2} {2\sigma^2} ~+~
\frac{x\mu_k} {\sigma^2} ~-~
\frac{\mu_k^2}{2\sigma^2} ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
*Since we are maximizing the equation* **over all possible classes of $k$**, *we can eliminate any terms that are indepedent of $k$.* This leaves us with

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
\frac{x\mu_k} {\sigma^2} ~-~
\frac{\mu_k^2}{2\sigma^2}
$$
which, when we rearrange the terms slightly, proves that maximizing the two equations are equivalent.

$$
log\left( p_k(x) \right)~=~\frac{x\mu_k}{\sigma^2}~-~\frac{\mu_k^2}{2\sigma^2}~+~log(\pi_k)~=~\delta_k(x)
$$


## 3

Following the exact same logic as the proof from question 2, we get to the point where we are looking to reduce the terms that are dependent on $k$. However, with Quadratic Discriminant Analysis, **we aren't making the assumption that there is a constant variance $\sigma^2$ across all classes of $k$.** Therefore, we can only eliminate the last term (normalizing constant), since all the other terms depend on $k$. Thus, the equation we are left with (shown below) is the final equation. The third term in the equation shows us that the entire formula does not change in a linear fashion as $x$ changes.
$$
~\delta_k(x) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma_k}} \right) ~-~
\frac{x^2} {2\sigma_k^2} ~+~
\frac{x\mu_k} {\sigma_k^2} ~-~
\frac{\mu_k^2}{2\sigma_k^2}
$$
## 4

**A**. Considering the fact that the variable $X$ is Uniformly Distributed, if we use observations that are within 0.10 of a test observation $x_i$, then, on average we will be using 10% of the observations (ignoring cases where $0.05\ge x_i\ge0.95$ for the sake of simplicity).

**B**. Given that we would like to use observations that are within 10% a test observation $x_i$ on both predictors $X_1$ and $X_2$, both of which are Uniformly Distributed over [0, 1], then on average we would only be using $0.1^2~=~0.01$ 1% of our observations.

**C**. Following the logical flow from the previous two question, if we have 100 predictors, all Uniformly Distributed over [0, 1], then the probability $P\left( (x_{i, j}~-~0.05) \ge X_j \ge (x_{i, j}~+~0.05) \right)~=~0.1$ for each predictor. Therefore, we would use roughly $0.1^{100}~=~1.00E^{-100}$ percent of our observations.

**D**. 
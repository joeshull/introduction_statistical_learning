# Conceptual

## 1

$$
P(X)~=~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}~~~and~~~\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{P(X)} {1~-~P(X)}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}} {1~-~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1 + \epsilon^{\beta_0~+~\beta_1X_1}}}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}} {\frac {1~+~\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}~-~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1 + \epsilon^{\beta_0~+~\beta_1X_1}}}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}} {\frac {1} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}}
$$


$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1~+~\epsilon^{\beta_0~+~\beta_1X_1}}~\dot~\frac {1~+~\epsilon^{\beta_0~+~\beta_1X_1}} {1}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\frac{\epsilon^{\beta_0~+~\beta_1X_1}} {1}
$$
$$
\epsilon^{\beta_0~+~\beta_1X_1}~=~\epsilon^{\beta_0~+~\beta_1X_1}
$$

## 2

Prove that maximizing the equation

$$
p_k(x)~=~\frac{\pi_k\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}} {\sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}}
$$
for $k$, is equivalent to maximizing the equation

$$
\delta_k(x)~=~\frac{x\mu_k}{\sigma^2}~-~\frac{\mu_k^3}{2\sigma^2}~+~log(\pi_k)
$$
for $k$.

As the text says, taking the $log$ of the first equation will get us started.

$$
log\left( p_k(x) \right)~=~log\left( \frac{\pi_k\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}} {\sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)}}\right)
$$

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{1}{2\sigma^2}\left( x~-~\mu_k^2 \right) ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
Expanding the third term in the equation we get

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{1}{2\sigma^2}\left( x^2~-~2x\mu_k~+~\mu_k^2 \right) ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
Once again expanding the third term, this time multiplying the $~-~\frac{1}{\sqrt{2\sigma62}}$ with all terms within the parenthesis, we get

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{x^2} {2\sigma^2} ~+~
\frac{2x\mu_k} {2\sigma^2} ~-~
\frac{\mu_k^2}{2\sigma^2} ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
simplifying what is now the fourth term in equation, we get

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma}} \right) ~-~
\frac{x^2} {2\sigma^2} ~+~
\frac{x\mu_k} {\sigma^2} ~-~
\frac{\mu_k^2}{2\sigma^2} ~-~
log\left( \sum_{l=1}^k\pi_l\cdot \frac{1}{\sqrt{2\pi\sigma}} \cdot    \epsilon^{~-~\frac{1}{2\sigma^2}\cdot\left( x~-~\mu_k^2\right)} \right)
$$
*Since we are maximizing the equation* **over all possible classes of $k$**, *we can eliminate any terms that are indepedent of $k$.* This leaves us with

$$
log\left( p_k(x) \right) ~=~ 
log(\pi_k) ~+~ 
\frac{x\mu_k} {\sigma^2} ~-~
\frac{\mu_k^2}{2\sigma^2}
$$
which, when we rearrange the terms slightly, proves that maximizing the two equations are equivalent.

$$
log\left( p_k(x) \right)~=~\frac{x\mu_k}{\sigma^2}~-~\frac{\mu_k^2}{2\sigma^2}~+~log(\pi_k)~=~\delta_k(x)
$$


## 3

Following the exact same logic as the proof from question 2, we get to the point where we are looking to reduce the terms that are dependent on $k$. However, with Quadratic Discriminant Analysis, **we aren't making the assumption that there is a constant variance $\sigma^2$ across all classes of $k$.** Therefore, we can only eliminate the last term (normalizing constant), since all the other terms depend on $k$. Thus, the equation we are left with (shown below) is the final equation. The third term in the equation shows us that the entire formula does not change in a linear fashion as $x$ changes.
$$
~\delta_k(x) ~=~ 
log(\pi_k) ~+~ 
log\left( \frac{1} {\sqrt{2\pi\sigma_k}} \right) ~-~
\frac{x^2} {2\sigma_k^2} ~+~
\frac{x\mu_k} {\sigma_k^2} ~-~
\frac{\mu_k^2}{2\sigma_k^2}
$$

## 4

* **A**. Considering the fact that the variable $X$ is Uniformly Distributed, if we use observations that are within 0.10 of a test observation $x_i$, then, on average we will be using 10% of the observations (ignoring cases where $0.05\ge x_i\ge0.95$ for the sake of simplicity).

* **B**. Given that we would like to use observations that are within 10% a test observation $x_i$ on both predictors $X_1$ and $X_2$, both of which are Uniformly Distributed over [0, 1], then on average we would only be using $0.1^2~=~0.01$ 1% of our observations.

* **C**. Following the logical flow from the previous two question, if we have 100 predictors, all Uniformly Distributed over [0, 1], then the probability $P\left( (x_{i, j}~-~0.05) \ge X_j \ge (x_{i, j}~+~0.05) \right)~=~0.1$ for each predictor. Therefore, we would use roughly $0.1^{100}~=~1.00E^{-100}$ percent of our observations.

* **D**. As we can see from parts **A** - **C**, as the number of predictors $p$ increases, the number of observations that we would consider "near" our test observations $x_i$ decreases drastically. Therefore, when using KNN for example, more likely than not there aren't going to be any observations that fit our definition of "near." More probable is that some of our observations will be "near" out test observation *with respect to some predictors*, and rather far away from our test observation with respect to other predictors, which then begs the question of whether we should use those "not-so-near" when predicting a quantitative or qualitative response of our test observation.

* **E**. If we would like to create a hypercube that has a sample space equal to 10% of the observations "nearest" to a test observation $x_i$, the length of each side of the hypercube will be equal to $0.1^{\frac{1}{p}}$, where $p$ is the number of predictors. The intution behind this, **given that we want the "size" (for lack of a more dimension-expansive term) of our our sample space to remain constant at 0.1**, is as follows; 

    * With $p$ being equal to 1, this will just be a vector of length 0.1, meaning that roughly 10% of our observations would be within 10% of the range of $X_1$closest to the test observation $x_i$.

    * For 2 dimensions, aka 2 predictors, our sample space is an area, and the formula for area is $A~=~l^2$ (given that we want the length of both vectors to be the same). Thus, the length of our vectors is equal to $l~=~\sqrt{0.1}~=~0.1^{\frac{1}{2}}~=~0.32$. This means that roughly 32% of the observations would be within 10% of the range of **one** of the predictors $X_1~or~X_2$ closest to the test observation, but still only 10% would be would be within 10% of the range of **both** predictors closest to the test observation.

    * We can see (below) that as our number of dimensions/predictors increases, in order for our sample space to remain constant at 0.1, the length of each vector of the hypercube asymptotically approaches 1. Since *each vector represents a predictor*, and **the length of each vector represents the percent of the observations that are within 10% of the range of ONE of the predictors $X_j$ closest to the test observation**, we can see that as $p$ increases, the percent of observations that are "close" to the test observation with respect to ONE of the predictors increases, yet the percent of observations that are "close" to the test observation with respect to ALL the predictors decreases.
    
    * In more broad terms, as the number of dimensions increases, the percent of observations that are "close" to one value of a test observation increases, but the percent of observations that are "close" to all values of a test observation decreases.
    
```{r}
# percent of observations that are close to ONE value of a test observation approaching 1
percents <- NULL
for (i in 1:100) {
    percent <- 0.1**(1/i)
    percents <- c(percents, percent)
}
print(percents[1:10])
```
    
## 5

* **A**. If the true Bayes decision boundary is linear, we would expect Linear Discriminant Analysis (LDA) to outperform Quadratic Discriminant Analysis (QDA) on both the training and test sets.

* **B**. If the true Bayes decision boundary is non-linear, we would expect QDA to outperform LDA on both the training and test sets.

* **C**. As the sample size $n$ increases, we would expect QDA to outperform LDA. The reason for this is that when the sample size is small, using LDA reduces the variance of the classifier far more than QDA, making it more generalizable to new data. With larger data sets, since reducing the **bias** is more important that reducing the variance, QDA will usually produce a better model.

* **D**. The blanket statement that QDA will outperform LDA, or more generally, more flexible methods will outperform less flexible methods **all** the time is false. Broadly speaking, when modeling data, you are trying to model the true relationship between a set of variables and a target variable. Therefore, the **best** model will be the model that most closely follows this true relationship. So, when the Bayes Decision Boundary is is truly linear, then QDA will not outperform LDA because QDA assumes the decision boundary is quadratic.

## 6

Since Logistic Regression follows the linear model, where the output $Y$ is the log odds of a success, defining the equation in terms of the problem context is step one.

$$Y~=~\beta_0~+~\beta_1X_1~+~\beta_2X_2$$

$$Recieve~an~A~=~intercept~+~\beta_1(hours~studied)~+~\beta_2(undergrad~GPA)$$

The estimates for $\beta_0,~\beta_1~and~\beta_2$ being equal to -6, 0.05 and 1, respectively. To get the *probability* that a student will recieve an A is a matter of plugging these coefficients into the equation, along with the values for hours studied and undergrad GPA for the student, and then converting the output into a probability.

$$Recieve~an~A~=~-6~+~0.05(hours~studied)~+~1(undergrad~GPA)$$

* **A**. 
$$Recieve~an~A~=~-6~+~0.05(40)~+~1(3.5)~=~-0.5$$

$$-0.5~=~Log~Odds~of~Success$$

$$\epsilon^{-0.5}~=Odds~of~Success$$

$$Probability(Success)~=~\frac{\epsilon^{-0.5}}{1~+~\epsilon^{-0.5}}~=~0.3775407~=~37.75\%$$

* **B**. To find the number of hours a student with a 3.5 GPA would need to have a 50% change of getting an A, re-working the equation is all that is needed.

$$Log~Odds~(A)~=~-6~+~0.05(hours~studied)~+~1(3.5)$$

$$Odds~(A)~=~\epsilon^{-6~+~0.05(hours~studied)~+~3.5}$$

$$P(A)~=~\frac{\epsilon^{-6~+~0.05(hours~studied)~+~3.5}}{1~+~\epsilon^{-6~+~0.05(hours~studied)~+~3.5}}$$

$$0.5~=~\frac{\epsilon^{-6~+~0.05(hours~studied)~+~3.5}}{1~+~\epsilon^{-6~+~0.05(hours~studied)~+~3.5}}$$

$$0.5(1~+~\epsilon^{-6~+~0.05(hours~studied)~+~3.5})~=\epsilon^{-6~+~0.05(hours~studied)~+~3.5}$$

$$0.5(1~+~\epsilon^{-2.5~+~0.05(hours~studied)})~=\epsilon^{-2.5+~0.05(hours~studied)}$$

$$0.5~+~0.5(\epsilon^{-2.5~+~0.05(hours~studied)})~=\epsilon^{-2.5+~0.05(hours~studied)}$$

$$0.5~+~0.5Z~=Z~where~Z~=~\epsilon^{-2.5~+~0.05(hours~studied)}$$

$$0.5=0.5Z~where~Z~=~\epsilon^{-2.5~+~0.05(hours~studied)}$$

$$0.5=0.5(\epsilon^{-2.5~+~0.05(hours~studied)})$$

$$1=\epsilon^{-2.5~+~0.05(hours~studied)}$$

$$log(1)=-2.5~+~0.05(hours~studied)$$

$$log(1)~+~2.5=0.05(hours~studied)$$

$$\frac{log(1)~+~2.5}{0.05}=hours~studied~=~50$$

## 7

Outlining the problem, the goal is to find the probability of company $X$ distributing a dividend given it had a 4% profit last year. This can be expressed as $P(~Div~|~4~)$. Bayes' Theorem can then be rewritten in the context of the problem as:

$$P(~Div~|~4~)~=~\frac{P(~4~|~Div~)P(~Div~)}{P(4)}$$

$$P(~Div~|~4~)~=~\frac{P(~4~|~Div~)P(~Div~)}{P(~4~|~Div~)P(~Div~)~+~P(~4~|~No~Div~)P(~No~Div~)}$$

The probability of a company returning a dividend $P(~Div~)~=~0.8$, therefore the probability that a company doesn't return a dividend is $P(~No~Div~)~=~0.2$. To find the probability that a company had a 4% profit last year given it did (and did not) return a dividend, we use the mean % profit of those that did return a dividend (10) and those that did not (0), and the standard deviation that defines the distributions ($\sqrt{36}~=~6$). Assuming a Normal distribution, the probability that a company had a 4% profit last year given they returned a dividend, $P(~4~|~Div~)$, is `dnorm(4, 10, 6) = 0.04032845`. The probability a company had a 4% profit given they did *not* return a dividend, $P(~4~|~No~Div~)$, is `dnorm(4, 0, 6) = 0.05324133`. Putting all these numbers into the above equation:

$$P(~Div~|~4~)~=~\frac{(0.04032845)(0.8)}{(0.04032845)(0.8)~+~(0.05324133)(0.2)}~=~0.7518525~=~75.2\%$$

## 8

Since the true evaluation of a model will always be on unseen data, otherwise known as the testing set, we need to compare the testing error rates of the two classification methods. The testing error rate for Logistic Regression is given as 30%.

To find the testing error rate using KNN where K = 1, a deconstruction of a weighted average is needed. However, since the training and testing data sets are equally divided (so their weights would both be 0.5), this will be equal to the arithmetic mean. When K = 1, the nearest neighbor to any given point is itself, and therefore the training error is 0%. So, to find $testing~error$ in 
$\bar{x}~=\frac{~training~error~+~testing~error}{2}$, when the known numbers are plugged in, $18~=\frac{0~+~testing~error}{2}$, the testing error comes out to be 36%. Therefore, since 30% < 36%, using Logistic Regression would provide a lower error rate on new observations.

## 9

$$Odds~=~\frac{P}{1~-~P}~~~and~~~Probability~=~\frac{Odds}{1~+~Odds}$$

* **A**.

$$Odds~=~\frac{P}{1~-~P}$$

$$0.37~=~\frac{P}{1~-~P}$$

$$0.37(1~-~P)~=~P$$

$$0.37~-~0.37P~=~P$$

$$0.37=~1.37P$$

$$\frac{0.37}{1.37}=~P~=~0.270073~=~27\%$$

* **B**. 

$$P~=~\frac{Odds}{1~+~Odds}$$

$$0.16~=~\frac{Odds}{1~+~Odds}$$

$$0.16(1~+~Odds)~=~Odds$$

$$0.16~+~0.16(Odds)~=~Odds$$

$$0.16~=~0.84(Odds)$$

$$\frac{0.16}{0.84}~=~Odds~=~0.1904762~=~0.19$$
